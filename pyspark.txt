%pyspark --jars /usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar

from pyspark.context import SparkContext
from pyspark import SparkConf
from pyspark.sql import Row
from pyspark.sql import HiveContext
from pyspark.sql import DataFrame

sc = SparkContext.getOrCreate()
hiveCtx = HiveContext(sc)


conf = {"dynamodb.servicename": "dynamodb", "dynamodb.input.tableName": "testdb_for_spark", "dynamodb.endpoint": "https://dynamodb.us-east-2.amazonaws.com", "dynamodb.regionid":"us-east-2", "mapred.input.format.class":"org.apache.hadoop.dynamodb.read.DynamoDBInputFormat","mapred.output.format.class":"org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat", "mapred.max.split.size": "4"}

# MapReduce: Maps a trained model and return format:[hashtag, [num_of_pos, num_of_neg]], then reduce
r = res.map(lambda x: (x[0], [x[1], x[2]])).reduceByKey(lambda m, n: [m[0] + n[0], m[1] + n[1]])

# Create dataframe and hive table
df = r.map(lambda x: Row(hashtag=x[0], pos=x[1][0], neg=x[1][1])).toDF(['hashtag', 'pos', 'neg'])
df.registerTempTable("temp_t")
aggRDD = hiveCtx.sql("select * from temp_t")
aggRDD.write.saveAsTable("test_result")



#orders  = sc.hadoopRDD(inputFormatClass="org.apache.hadoop.dynamodb.read.DynamoDBInputFormat", keyClass="org.apache.hadoop.io.Text", #valueClass="org.apache.hadoop.dynamodb.DynamoDBItemWritable", conf=conf)

#items = tweets.map(lambda x: [x[1]['item']['text']['s'], [h['s'] for h in x[1]['item']['hashtags']['l']], x[1]['item']['id']['s']])

