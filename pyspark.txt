%pyspark --jars /usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar

from pyspark.context import SparkContext
from pyspark import SparkConf

sc = SparkContext.getOrCreate()

conf = {"dynamodb.servicename": "dynamodb", "dynamodb.input.tableName": "testdb_for_spark", "dynamodb.endpoint": "https://dynamodb.us-east-2.amazonaws.com", "dynamodb.regionid":"us-east-2", "mapred.input.format.class":"org.apache.hadoop.dynamodb.read.DynamoDBInputFormat","mapred.output.format.class":"org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat", "mapred.max.split.size": "4"}

orders  = sc.hadoopRDD(inputFormatClass="org.apache.hadoop.dynamodb.read.DynamoDBInputFormat", keyClass="org.apache.hadoop.io.Text", valueClass="org.apache.hadoop.dynamodb.DynamoDBItemWritable", conf=conf)

items = tweets.map(lambda x: [x[1]['item']['text']['s'], [h['s'] for h in x[1]['item']['hashtags']['l']], x[1]['item']['id']['s']])

res = items.filter(lambda x: 'final' in x[1])
res.count()
